#      fm[1,5]=I15
#      fm[2,3]=I23
#      fm[2,4]=I24
#      fm[2,5]=I25
#      fm[3,4]=I34
#      fm[3,5]=I35
#      fm[4,5]=I45
#
#      for(i in 1:5){
#        for(j in 1:5){
#          fm[j,i]=fm[i,j]
#        }
#      }
#      fminv=solve(fm)
#
#      smaxtem[kk]=(scor^2)*fminv[5,5]
#
#    }
#    # MAX statistic
#
#    smax[bbb]=max(smaxtem)
#
#
# # tradition
#
#    # score function
#
#    score_tra=sum(data_y*data_x[,2]*data_x[,3]-(exp(beta_0est+beta_1est*data_x[,1]+beta_2est*data_x[,2]+beta_3est*data_x[,3])*data_x[,2]*data_x[,3]/(1+exp(beta_0est+beta_1est*data_x[,1]+beta_2est*data_x[,2]+beta_3est*data_x[,3]))))
#
#    # Fisher infor
#
#    I11_tra=sum(exp(beta_0est+beta_1est*data_x[,1]+beta_2est*data_x[,2]+beta_3est*data_x[,3])/((1+exp(beta_0est+beta_1est*data_x[,1]+beta_2est*data_x[,2]+beta_3est*data_x[,3]))^2))
#    I22_tra=sum((data_x[,1]^2)*exp(beta_0est+beta_1est*data_x[,1]+beta_2est*data_x[,2]+beta_3est*data_x[,3])/((1+exp(beta_0est+beta_1est*data_x[,1]+beta_2est*data_x[,2]+beta_3est*data_x[,3]))^2))
#    I33_tra=sum((data_x[,2]^2)*exp(beta_0est+beta_1est*data_x[,1]+beta_2est*data_x[,2]+beta_3est*data_x[,3])/((1+exp(beta_0est+beta_1est*data_x[,1]+beta_2est*data_x[,2]+beta_3est*data_x[,3]))^2))
#    I44_tra=sum((data_x[,3]^2)*exp(beta_0est+beta_1est*data_x[,1]+beta_2est*data_x[,2]+beta_3est*data_x[,3])/((1+exp(beta_0est+beta_1est*data_x[,1]+beta_2est*data_x[,2]+beta_3est*data_x[,3]))^2))
#    I55_tra=sum(((data_x[,2]*data_x[,3])^2)*exp(beta_0est+beta_1est*data_x[,1]+beta_2est*data_x[,2]+beta_3est*data_x[,3])/((1+exp(beta_0est+beta_1est*data_x[,1]+beta_2est*data_x[,2]+beta_3est*data_x[,3]))^2))
#    I12_tra=sum(data_x[,1]*exp(beta_0est+beta_1est*data_x[,1]+beta_2est*data_x[,2]+beta_3est*data_x[,3])/((1+exp(beta_0est+beta_1est*data_x[,1]+beta_2est*data_x[,2]+beta_3est*data_x[,3]))^2))
#    I13_tra=sum(data_x[,2]*exp(beta_0est+beta_1est*data_x[,1]+beta_2est*data_x[,2]+beta_3est*data_x[,3])/((1+exp(beta_0est+beta_1est*data_x[,1]+beta_2est*data_x[,2]+beta_3est*data_x[,3]))^2))
#    I14_tra=sum(data_x[,3]*exp(beta_0est+beta_1est*data_x[,1]+beta_2est*data_x[,2]+beta_3est*data_x[,3])/((1+exp(beta_0est+beta_1est*data_x[,1]+beta_2est*data_x[,2]+beta_3est*data_x[,3]))^2))
#    I15_tra=sum((data_x[,2]*data_x[,3])*exp(beta_0est+beta_1est*data_x[,1]+beta_2est*data_x[,2]+beta_3est*data_x[,3])/((1+exp(beta_0est+beta_1est*data_x[,1]+beta_2est*data_x[,2]+beta_3est*data_x[,3]))^2))
#    I23_tra=sum((data_x[,1]*data_x[,2])*exp(beta_0est+beta_1est*data_x[,1]+beta_2est*data_x[,2]+beta_3est*data_x[,3])/((1+exp(beta_0est+beta_1est*data_x[,1]+beta_2est*data_x[,2]+beta_3est*data_x[,3]))^2))
#    I24_tra=sum((data_x[,1]*data_x[,3])*exp(beta_0est+beta_1est*data_x[,1]+beta_2est*data_x[,2]+beta_3est*data_x[,3])/((1+exp(beta_0est+beta_1est*data_x[,1]+beta_2est*data_x[,2]+beta_3est*data_x[,3]))^2))
#    I25_tra=sum((data_x[,1]*data_x[,2]*data_x[,3])*exp(beta_0est+beta_1est*data_x[,1]+beta_2est*data_x[,2]+beta_3est*data_x[,3])/((1+exp(beta_0est+beta_1est*data_x[,1]+beta_2est*data_x[,2]+beta_3est*data_x[,3]))^2))
#    I34_tra=sum((data_x[,2]*data_x[,3])*exp(beta_0est+beta_1est*data_x[,1]+beta_2est*data_x[,2]+beta_3est*data_x[,3])/((1+exp(beta_0est+beta_1est*data_x[,1]+beta_2est*data_x[,2]+beta_3est*data_x[,3]))^2))
#    I35_tra=sum((data_x[,2]*data_x[,2]*data_x[,3])*exp(beta_0est+beta_1est*data_x[,1]+beta_2est*data_x[,2]+beta_3est*data_x[,3])/((1+exp(beta_0est+beta_1est*data_x[,1]+beta_2est*data_x[,2]+beta_3est*data_x[,3]))^2))
#    I45_tra=sum((data_x[,3]*data_x[,2]*data_x[,3])*exp(beta_0est+beta_1est*data_x[,1]+beta_2est*data_x[,2]+beta_3est*data_x[,3])/((1+exp(beta_0est+beta_1est*data_x[,1]+beta_2est*data_x[,2]+beta_3est*data_x[,3]))^2))
#
#    fm_tra=matrix(NA,nrow=5,ncol=5)
#    fm_tra[1,1]=I11_tra
#    fm_tra[2,2]=I22_tra
#    fm_tra[3,3]=I33_tra
#    fm_tra[4,4]=I44_tra
#    fm_tra[5,5]=I55_tra
#    fm_tra[1,2]=I12_tra
#    fm_tra[1,3]=I13_tra
#    fm_tra[1,4]=I14_tra
#    fm_tra[1,5]=I15_tra
#    fm_tra[2,3]=I23_tra
#    fm_tra[2,4]=I24_tra
#    fm_tra[2,5]=I25_tra
#    fm_tra[3,4]=I34_tra
#    fm_tra[3,5]=I35_tra
#    fm_tra[4,5]=I45_tra
#    for(i in 1:5){
#      for(j in 1:5){
#        fm_tra[j,i]=fm_tra[i,j]
#      }
#    }
#
#    fminv_tra=solve(fm_tra)
#    sc[bbb]=(score_tra^2)*fminv_tra[5,5]
################################# test global term ################################################
glm.sim=glm(data_y~data_xnew2[,1],data=data_xnew2,family="binomial")
beta_0est=summary(glm.sim)$coefficients[1,1]
beta_1est=summary(glm.sim)$coefficients[2,1]
smaxtem=c()
for(kk in 1:length(theta)){
# Score function
scor=matrix(NA,nrow=3,ncol=1)
scor[1,1]=sum(data_y*eex-exp(beta_0est+beta_1est*cova)*eex/(1+exp(beta_0est+beta_1est*cova)))
scor[2,1]=sum(data_y*(dum1+theta[kk]*dum2)-exp(beta_0est+beta_1est*cova)*(dum1+theta[kk]*dum2)/(1+exp(beta_0est+beta_1est*cova)))
scor[3,1]=sum(data_y*(dum1e+theta[kk]*dum2e)-exp(beta_0est+beta_1est*cova)*(dum1e+theta[kk]*dum2e)/(1+exp(beta_0est+beta_1est*cova)))
# Informatic metric
I11=sum(exp(beta_0est+beta_1est*cova)/((1+exp(beta_0est+beta_1est*cova))^2))
I22=sum(exp(beta_0est+beta_1est*cova)*(cova^2)/((1+exp(beta_0est+beta_1est*cova))^2))
I33=sum(exp(beta_0est+beta_1est*cova)*(eex^2)/((1+exp(beta_0est+beta_1est*cova))^2))
I44=sum(((dum1+theta[kk]*dum2)^2)*exp(beta_0est+beta_1est*cova)/((1+exp(beta_0est+beta_1est*cova))^2))
I55=sum(((dum1e+theta[kk]*dum2e)^2)*exp(beta_0est+beta_1est*cova)/((1+exp(beta_0est+beta_1est*cova))^2))
I12=sum(exp(beta_0est+beta_1est*cova)*cova/((1+exp(beta_0est+beta_1est*cova))^2))
I13=sum(exp(beta_0est+beta_1est*cova)*eex/((1+exp(beta_0est+beta_1est*cova))^2))
I14=sum(exp(beta_0est+beta_1est*cova)*(dum1+theta[kk]*dum2)/((1+exp(beta_0est+beta_1est*cova))^2))
I15=sum(exp(beta_0est+beta_1est*cova)*(dum1e+theta[kk]*dum2e)/((1+exp(beta_0est+beta_1est*cova))^2))
I23=sum(exp(beta_0est+beta_1est*cova)*cova*eex/((1+exp(beta_0est+beta_1est*cova))^2))
I24=sum(exp(beta_0est+beta_1est*cova)*cova*(dum1+theta[kk]*dum2)/((1+exp(beta_0est+beta_1est*cova))^2))
I25=sum(exp(beta_0est+beta_1est*cova)*cova*(dum1e+theta[kk]*dum2e)/((1+exp(beta_0est+beta_1est*cova))^2))
I34=sum(exp(beta_0est+beta_1est*cova)*eex*(dum1+theta[kk]*dum2)/((1+exp(beta_0est+beta_1est*cova))^2))
I35=sum(exp(beta_0est+beta_1est*cova)*eex*(dum1e+theta[kk]*dum2e)/((1+exp(beta_0est+beta_1est*cova))^2))
I45=sum(exp(beta_0est+beta_1est*cova)*(dum1+theta[kk]*dum2)*(dum1e+theta[kk]*dum2e)/((1+exp(beta_0est+beta_1est*cova))^2))
fm=matrix(NA,nrow=5,ncol=5)
fm[1,1]=I11
fm[2,2]=I22
fm[3,3]=I33
fm[4,4]=I44
fm[5,5]=I55
fm[1,2]=I12
fm[1,3]=I13
fm[1,4]=I14
fm[1,5]=I15
fm[2,3]=I23
fm[2,4]=I24
fm[2,5]=I25
fm[3,4]=I34
fm[3,5]=I35
fm[4,5]=I45
for(i in 1:5){
for(j in 1:5){
fm[j,i]=fm[i,j]
}
}
fminv=solve(fm)
smaxtem[kk]=t(scor)%*%fminv[3:5,3:5]%*%scor
}
# MAX statistic
smax[bbb]=max(smaxtem)
# tradition
# score function
score_tra=matrix(NA,nrow=3,ncol=1)
score_tra[1,1]=sum(data_y*data_x[,2]-(exp(beta_0est+beta_1est*data_x[,1])*data_x[,2]/(1+exp(beta_0est+beta_1est*data_x[,1]))))
score_tra[2,1]=sum(data_y*data_x[,3]-(exp(beta_0est+beta_1est*data_x[,1])*data_x[,3]/(1+exp(beta_0est+beta_1est*data_x[,1]))))
score_tra[3,1]=sum(data_y*data_x[,2]*data_x[,3]-(exp(beta_0est+beta_1est*data_x[,1])*data_x[,2]*data_x[,3]/(1+exp(beta_0est+beta_1est*data_x[,1]))))
# Fisher infor
I11_tra=sum(exp(beta_0est+beta_1est*data_x[,1])/((1+exp(beta_0est+beta_1est*data_x[,1]))^2))
I22_tra=sum((data_x[,1]^2)*exp(beta_0est+beta_1est*data_x[,1])/((1+exp(beta_0est+beta_1est*data_x[,1]))^2))
I33_tra=sum((data_x[,2]^2)*exp(beta_0est+beta_1est*data_x[,1])/((1+exp(beta_0est+beta_1est*data_x[,1]))^2))
I44_tra=sum((data_x[,3]^2)*exp(beta_0est+beta_1est*data_x[,1])/((1+exp(beta_0est+beta_1est*data_x[,1]))^2))
I55_tra=sum(((data_x[,2]*data_x[,3])^2)*exp(beta_0est+beta_1est*data_x[,1])/((1+exp(beta_0est+beta_1est*data_x[,1]))^2))
I12_tra=sum(data_x[,1]*exp(beta_0est+beta_1est*data_x[,1])/((1+exp(beta_0est+beta_1est*data_x[,1]))^2))
I13_tra=sum(data_x[,2]*exp(beta_0est+beta_1est*data_x[,1])/((1+exp(beta_0est+beta_1est*data_x[,1]))^2))
I14_tra=sum(data_x[,3]*exp(beta_0est+beta_1est*data_x[,1])/((1+exp(beta_0est+beta_1est*data_x[,1]))^2))
I15_tra=sum((data_x[,2]*data_x[,3])*exp(beta_0est+beta_1est*data_x[,1])/((1+exp(beta_0est+beta_1est*data_x[,1]))^2))
I23_tra=sum((data_x[,1]*data_x[,2])*exp(beta_0est+beta_1est*data_x[,1])/((1+exp(beta_0est+beta_1est*data_x[,1]))^2))
I24_tra=sum((data_x[,1]*data_x[,3])*exp(beta_0est+beta_1est*data_x[,1])/((1+exp(beta_0est+beta_1est*data_x[,1]))^2))
I25_tra=sum((data_x[,1]*data_x[,2]*data_x[,3])*exp(beta_0est+beta_1est*data_x[,1])/((1+exp(beta_0est+beta_1est*data_x[,1]))^2))
I34_tra=sum((data_x[,2]*data_x[,3])*exp(beta_0est+beta_1est*data_x[,1])/((1+exp(beta_0est+beta_1est*data_x[,1]))^2))
I35_tra=sum((data_x[,2]*data_x[,2]*data_x[,3])*exp(beta_0est+beta_1est*data_x[,1])/((1+exp(beta_0est+beta_1est*data_x[,1]))^2))
I45_tra=sum((data_x[,3]*data_x[,2]*data_x[,3])*exp(beta_0est+beta_1est*data_x[,1])/((1+exp(beta_0est+beta_1est*data_x[,1]))^2))
fm_tra=matrix(NA,nrow=5,ncol=5)
fm_tra[1,1]=I11_tra
fm_tra[2,2]=I22_tra
fm_tra[3,3]=I33_tra
fm_tra[4,4]=I44_tra
fm_tra[5,5]=I55_tra
fm_tra[1,2]=I12_tra
fm_tra[1,3]=I13_tra
fm_tra[1,4]=I14_tra
fm_tra[1,5]=I15_tra
fm_tra[2,3]=I23_tra
fm_tra[2,4]=I24_tra
fm_tra[2,5]=I25_tra
fm_tra[3,4]=I34_tra
fm_tra[3,5]=I35_tra
fm_tra[4,5]=I45_tra
for(i in 1:5){
for(j in 1:5){
fm_tra[j,i]=fm_tra[i,j]
}
}
fminv_tra=solve(fm_tra)
sc[bbb]=t(score_tra)%*%fminv_tra[3:5,3:5]%*%score_tra
}
}
# proc.time()-ptm
# # calculate power (interaction)
#
# if(length(which(is.na(smax)))==0 & length(which(is.na(sc)))==0){
#
#   power.smax=length(which(smax>=scv))/itertem
#   power.sc=length(which(sc>=qchisq(1-alpha,1)))/itertem
#
#
# }else{
#
#   power.smax=length(which(smax[-which(is.na(smax))]>=scv))/length(smax[-which(is.na(smax))])
#   power.sc=length(which(sc[-which(is.na(sc))]>=qchisq(1-alpha,1)))/length(sc[-which(is.na(sc))])
#
# }
# calculate power (global)
if(length(which(is.na(smax)))==0 & length(which(is.na(sc)))==0){
power.smax=length(which(smax>=scv))/itertem
power.sc=length(which(sc>=qchisq(1-alpha,3)))/itertem
}else{
power.smax=length(which(smax[-which(is.na(smax))]>=scv))/length(smax[-which(is.na(smax))])
power.sc=length(which(sc[-which(is.na(sc))]>=qchisq(1-alpha,3)))/length(sc[-which(is.na(sc))])
}
result[1,hh]=power.smax
result[2,hh]=power.sc
}
proc.time()-ptm
View(result)
setwd("J:/Onedirve/OneDrive/Documents/result/cob2/X_5_Y_4")
alpha<- read.xlsx(file = "result_alpha.xlsx",1,header = F)
beta1<-read.xlsx(file = "result_b1.xlsx",1,header = F)
beta2<-read.xlsx(file = "result_b2.xlsx",1,header = F)
beta3<-read.xlsx(file = "result_b3.xlsx",1,header = F)
result<-cbind(alpha,beta1,beta2,beta3)
rownames(result)=c("T1emp", "T2emp", "T3emp","CobT1_y~x","CobT2_y~x","CobT1","CobT2",
"Y~X linear","Y~X catego","X~Y linear","X~Y catego")
colnames(result)=c("Null","Linear","Nonlinear","Nonmontonic")
write.xlsx(x = result, file = "C:/Users/o0/Desktop/ordinal data/simulation/Combi_ordinal/result/result.xlsx",
sheetName = "result", row.names =TRUE,col.names = TRUE)
library(xlsx)
setwd("J:/Onedirve/OneDrive/Documents/result/cob2/X_5_Y_4")
alpha<- read.xlsx(file = "result_alpha.xlsx",1,header = F)
beta1<-read.xlsx(file = "result_b1.xlsx",1,header = F)
beta2<-read.xlsx(file = "result_b2.xlsx",1,header = F)
beta3<-read.xlsx(file = "result_b3.xlsx",1,header = F)
result<-cbind(alpha,beta1,beta2,beta3)
rownames(result)=c("T1emp", "T2emp", "T3emp","CobT1_y~x","CobT2_y~x","CobT1","CobT2",
"Y~X linear","Y~X catego","X~Y linear","X~Y catego")
colnames(result)=c("Null","Linear","Nonlinear","Nonmontonic")
write.xlsx(x = result, file = "C:/Users/o0/Desktop/ordinal data/simulation/Combi_ordinal/result/result.xlsx",
sheetName = "result", row.names =TRUE,col.names = TRUE)
min(0,2,0.4)
N = 500
alphay = c(-1, 0, 1)
betay = -1
alphax = c(-1, 0, 1, 2)
betax = 1
eta0 = rep(0,5)
eta1 = 0.2 * (-2:2)
eta2= c(-.3, .18, .20, .22, .24)
eta3 = 0.1 * c(-2,0,2,0,-2)
sim0s = ordinalsim(alphax, betax, alphay, betay, eta0, N, Nemp, NREPL)
data = generate.data(alphax, betax, alphay, betay, eta0, N)
COBOT.emp(data, Nemp)
setwd("C:/Users/o0/Desktop/ordinal data/simulation/Combi_ordinal")
#### Need all functions defined in COBOT-analysis.r
source("cobot-analysis.r")
COBOT.emp(data, Nemp)
Nemp = 100
COBOT.emp(data, Nemp)
nx = length(table(data$x))
data = generate.data(alphax, betax, alphay, betay, eta0, N)
#### Function for simulating data
generate.data = function(alphax, betax, alphay, betay, eta, N) {
z = rnorm(N,0,1)
x = y = numeric(N)
## px is an N x length(alphax) matrix.
## Each row has the TRUE cummulative probabilities for each subject.
px = (1 + exp(- outer(alphax, betax*z, "+"))) ^ (-1)
aa = runif(N)
for(i in 1:N)
x[i] = sum(aa[i] > px[,i])
x = as.numeric(as.factor(x))
## x = x+1 may have category gaps if there are small probability categories.
py = (1 + exp(- outer(alphay, betay*z+eta[x], "+"))) ^ (-1)
aa = runif(N)
for(i in 1:N)
y[i] = sum(aa[i] > py[,i])
y = as.numeric(as.factor(y))
## y = y+1 may have category gaps if there are small probability categories.
return(list(x=x, y=y, z=z))
}
data = generate.data(alphax, betax, alphay, betay, eta0, N)
COBOT.emp(data, Nemp)
COBOT.stat(data, nx, ny)
nx = length(table(data$x))
ny = length(table(data$y))
COBOT.stat(data, nx, ny)
sim1s = ordinalsim(alphax, betax, alphay, betay, eta1, N, Nemp, NREPL)
ordinalsim = function(alphax, betax, alphay, betay, eta, N, Nemp, NREPL) {
require(rms)
pval.emp = matrix(,NREPL,11)
for (ii in 1:NREPL) {
data = generate.data(alphax, betax, alphay, betay, eta0, N)
pval.emp[ii,] = COBOT.emp(data, Nemp)
}
param = list(alpha.yz=alphay, beta.yz=betay,
alpha.xz=alphax, beta.xz=betax, eta=eta,
N=N, Nemp=Nemp, Nrepl=NREPL)
list(par= param, pval = pval.emp)
}
sim1s = ordinalsim(alphax, betax, alphay, betay, eta1, N, Nemp, NREPL)
NREPL = 10
sim0s = ordinalsim(alphax, betax, alphay, betay, eta0, N, Nemp, NREPL)
beta=matrix(0,11,3)
beta[,1]<-apply(sim1s$pval<p,2,mean,rm.na=T)
sim1s = ordinalsim(alphax, betax, alphay, betay, eta1, N, Nemp, NREPL)
beta=matrix(0,11,3)
beta[,1]<-apply(sim1s$pval<p,2,mean,rm.na=T)
p=0.05
beta[,1]<-apply(sim1s$pval<p,2,mean,rm.na=T)
#### Simulation programms for testing for association between two
#### ordinal variables while adjusting for covariates.
####
#### Chun Li
#### October 15, 2009
#### Chun Li and Bryan E. Shepherd (2009) Test of Association between
#### Two Ordinal Variables while Adjusting for Covariates. JASA (in press)
#### Here we assume proportional odds relationship between y and Z and
#### between x and Z.  In principle, any other multinomial regression
#### analysis can be used in place of proportional odds models.
#### R library requirement:
## rms package is needed for the lrm() function.
#### functions defined in addition to those in COBOT-analysis.r :
## isotonic():        proportional odds model with an isotonic procedure
## generate.data():   data generation
## ordinalsim():      main function of simulation
#### Data requirements: [COBOT.stat(), COBOT.emp(), COBOT.scores(), isotonic()]
## The argument "data" must be a list with three elements:
##   y: a vector with integer values representing levels (1 to ny)
##   x: a vector with integer values representing levels (1 to nx)
##   z: a vector or matrix of numerical covariate values
## The lengths of y, x, z (or nrow(z) if matrix) must be the same.
#S1
setwd("C:/Users/o0/Desktop/ordinal data/simulation/Combi_ordinal")
#### Need all functions defined in COBOT-analysis.r
source("cobot-analysis.r")
ptm<-proc.time()
#### Isotonic regression
#### Function for simulating data
generate.data = function(alphax, betax, alphay, betay, eta, N) {
z = rnorm(N,0,1)
x = y = numeric(N)
## px is an N x length(alphax) matrix.
## Each row has the TRUE cummulative probabilities for each subject.
px = (1 + exp(- outer(alphax, betax*z, "+"))) ^ (-1)
aa = runif(N)
for(i in 1:N)
x[i] = sum(aa[i] > px[,i])
x = as.numeric(as.factor(x))
## x = x+1 may have category gaps if there are small probability categories.
py = (1 + exp(- outer(alphay, betay*z+eta[x], "+"))) ^ (-1)
aa = runif(N)
for(i in 1:N)
y[i] = sum(aa[i] > py[,i])
y = as.numeric(as.factor(y))
## y = y+1 may have category gaps if there are small probability categories.
return(list(x=x, y=y, z=z))
}
#### Function for simulation
ordinalsim = function(alphax, betax, alphay, betay, eta, N, Nemp, NREPL) {
require(rms)
pval.emp = matrix(,NREPL,11)
for (ii in 1:NREPL) {
data = generate.data(alphax, betax, alphay, betay, eta0, N)
pval.emp[ii,] = COBOT.emp(data, Nemp)
}
param = list(alpha.yz=alphay, beta.yz=betay,
alpha.xz=alphax, beta.xz=betax, eta=eta,
N=N, Nemp=Nemp, Nrepl=NREPL)
list(par= param, pval = pval.emp)
}
#### Start simulations
ptm=proc.time()
N = 500
Nemp = 100
NREPL = 10
# 1. X_5. Y_4
alphay = c(-1, 0, 1)
betay = -1
alphax = c(-1, 0, 1, 2)
betax = 1
eta0 = rep(0,5)
eta1 = 0.2 * (-2:2)
eta2= c(-.3, .18, .20, .22, .24)
eta3 = 0.1 * c(-2,0,2,0,-2)
# #2. X_10. Y_4
# alphay = c(-1, 0, 1)
# betay = -.5
# alphax = c(-4:4)
# betax = 1
# eta0 = rep(0,10)
# eta1 = 0.2 * (-4:5)
# eta2= c(-.65,-.54,-.3, .18, .20, .22, .24,.34,.36,.45)
# eta3 = 0.1 * c(-2,0,2,0,-2,-2,0,2,0,-2)
# #3 X_7,Y_4
# alphay = c(-1, 0, 1)
# betay = -.5
# alphax = c(-2:3)
# betax = 1
# eta0 = rep(0,7)
# eta1 = 0.2 * (-3:3)
# eta2= c(-.3, .18, .20, .22, .24,.34,.45)
# eta3 = 0.1 * c(0,2,0,-2,-2,0,2)
# ### 4 X_3Y_4
# alphay = c(-1, 0, 1)
# betay = -.5
# alphax = c(0,1)
# betax = 1
# eta0 = rep(0,3)
# eta1 = 0.2 * (-1:1)
# eta2= c(-.3, .18, .20)
# eta3 = 0.1 * c(-2,0,2)
# #null distribution
sim0s = ordinalsim(alphax, betax, alphay, betay, eta0, N, Nemp, NREPL)
#### Simulation programms for testing for association between two
#### ordinal variables while adjusting for covariates.
####
#### Chun Li
#### October 15, 2009
#### Chun Li and Bryan E. Shepherd (2009) Test of Association between
#### Two Ordinal Variables while Adjusting for Covariates. JASA (in press)
#### Here we assume proportional odds relationship between y and Z and
#### between x and Z.  In principle, any other multinomial regression
#### analysis can be used in place of proportional odds models.
#### R library requirement:
## rms package is needed for the lrm() function.
#### functions defined in addition to those in COBOT-analysis.r :
## isotonic():        proportional odds model with an isotonic procedure
## generate.data():   data generation
## ordinalsim():      main function of simulation
#### Data requirements: [COBOT.stat(), COBOT.emp(), COBOT.scores(), isotonic()]
## The argument "data" must be a list with three elements:
##   y: a vector with integer values representing levels (1 to ny)
##   x: a vector with integer values representing levels (1 to nx)
##   z: a vector or matrix of numerical covariate values
## The lengths of y, x, z (or nrow(z) if matrix) must be the same.
#S1
setwd("C:/Users/o0/Desktop/ordinal data/simulation/Combi_ordinal")
#### Need all functions defined in COBOT-analysis.r
source("cobot-analysis.r")
ptm<-proc.time()
#### Isotonic regression
#### Function for simulating data
generate.data = function(alphax, betax, alphay, betay, eta, N) {
z = rnorm(N,0,1)
x = y = numeric(N)
## px is an N x length(alphax) matrix.
## Each row has the TRUE cummulative probabilities for each subject.
px = (1 + exp(- outer(alphax, betax*z, "+"))) ^ (-1)
aa = runif(N)
for(i in 1:N)
x[i] = sum(aa[i] > px[,i])
x = as.numeric(as.factor(x))
## x = x+1 may have category gaps if there are small probability categories.
py = (1 + exp(- outer(alphay, betay*z+eta[x], "+"))) ^ (-1)
aa = runif(N)
for(i in 1:N)
y[i] = sum(aa[i] > py[,i])
y = as.numeric(as.factor(y))
## y = y+1 may have category gaps if there are small probability categories.
return(list(x=x, y=y, z=z))
}
#### Function for simulation
ordinalsim = function(alphax, betax, alphay, betay, eta, N, Nemp, NREPL) {
require(rms)
pval.emp = matrix(,NREPL,11)
for (ii in 1:NREPL) {
data = generate.data(alphax, betax, alphay, betay, eta0, N)
pval.emp[ii,] = COBOT.emp(data, Nemp)
}
param = list(alpha.yz=alphay, beta.yz=betay,
alpha.xz=alphax, beta.xz=betax, eta=eta,
N=N, Nemp=Nemp, Nrepl=NREPL)
list(par= param, pval = pval.emp)
}
#### Start simulations
ptm=proc.time()
N = 500
Nemp = 10
NREPL = 10
# 1. X_5. Y_4
alphay = c(-1, 0, 1)
betay = -1
alphax = c(-1, 0, 1, 2)
betax = 1
eta0 = rep(0,5)
eta1 = 0.2 * (-2:2)
eta2= c(-.3, .18, .20, .22, .24)
eta3 = 0.1 * c(-2,0,2,0,-2)
# #2. X_10. Y_4
# alphay = c(-1, 0, 1)
# betay = -.5
# alphax = c(-4:4)
# betax = 1
# eta0 = rep(0,10)
# eta1 = 0.2 * (-4:5)
# eta2= c(-.65,-.54,-.3, .18, .20, .22, .24,.34,.36,.45)
# eta3 = 0.1 * c(-2,0,2,0,-2,-2,0,2,0,-2)
# #3 X_7,Y_4
# alphay = c(-1, 0, 1)
# betay = -.5
# alphax = c(-2:3)
# betax = 1
# eta0 = rep(0,7)
# eta1 = 0.2 * (-3:3)
# eta2= c(-.3, .18, .20, .22, .24,.34,.45)
# eta3 = 0.1 * c(0,2,0,-2,-2,0,2)
# ### 4 X_3Y_4
# alphay = c(-1, 0, 1)
# betay = -.5
# alphax = c(0,1)
# betax = 1
# eta0 = rep(0,3)
# eta1 = 0.2 * (-1:1)
# eta2= c(-.3, .18, .20)
# eta3 = 0.1 * c(-2,0,2)
# #null distribution
sim0s = ordinalsim(alphax, betax, alphay, betay, eta0, N, Nemp, NREPL)
proc.time()-ptm
#linear
sim1s = ordinalsim(alphax, betax, alphay, betay, eta1, N, Nemp, NREPL)
#nonlinear and monoto
sim2s = ordinalsim(alphax, betax, alphay, betay, eta2, N, Nemp, NREPL)
beta=matrix(0,11,3)
beta[,1]<-apply(sim1s$pval<p,2,mean,rm.na=T)
View(beta)
data = generate.data(alphax, betax, alphay, betay, eta0, N)
COBOT.emp(data, Nemp)
Nemp = 100
COBOT.emp(data, Nemp)
